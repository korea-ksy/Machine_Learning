# 🐌 **Logistic Regression 실습 & Confusion matrix**

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import datasets, model_selection, linear_model
from sklearn.metrics import mean_squared_error
```

## 🐌 **1-1. (미국 보스턴의 주택 가격) 데이터 읽어들이기 & Binary label 만들어주기**

```python
df_data = pd.read_excel('/content/drive/MyDrive/카카오 sw ai 중급 2(part5)/실습 파일/2. Scikit-learn (Answer)/boston_house_data.xlsx', index_col=0)
df_data.head()
```

|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8** | **9** | **10** | **11** | **12** |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **0** | 0.00632 | 18.0 | 2.31 | 0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1 | 296 | 15.3 | 396.90 | 4.98 |
| **1** | 0.02731 | 0.0 | 7.07 | 0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2 | 242 | 17.8 | 396.90 | 9.14 |
| **2** | 0.02729 | 0.0 | 7.07 | 0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2 | 242 | 17.8 | 392.83 | 4.03 |
| **3** | 0.03237 | 0.0 | 2.18 | 0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3 | 222 | 18.7 | 394.63 | 2.94 |
| **4** | 0.06905 | 0.0 | 2.18 | 0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3 | 222 | 18.7 | 396.90 | 5.33 |

```python
df_target = pd.read_excel('/content/drive/MyDrive/카카오 sw ai 중급 2(part5)/실습 파일/2. Scikit-learn (Answer)/boston_house_target.xlsx', index_col=0)
df_target.head()
```

|  | **0** |
| --- | --- |
| **0** | 24.0 |
| **1** | 21.6 |
| **2** | 34.7 |
| **3** | 33.4 |
| **4** | 36.2 |

```python
# 집값의 평균값이 얼마일까요?
mean_price = df_target[0].mean()
mean_price

>>> 22.532806324110677
```

```python
df_target['Label'] = df_target[0].apply(lambda x: 1 if x > mean_price else 0 ) # 새로운 함수를 '적용'해주려면?
df_target.head()

#위에서 구한 평균값보다 크면 1 작으면 0 
```

## 🐌 **1-2. Dataframe 을 Numpy array (배열, 행렬)로 바꿔주기**

```python
boston_data = np.array(df_data)
boston_target = np.array(df_target['Label'])
boston_target

array([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,
       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,
       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,
       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,
       0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]) 
```

## 🐌 **2. Feature 선택하기**

- 0 : **범죄율**
- 1 : **25,000 평방피트를 초과하는 거주지역 비율**
- 2 : **비소매상업지역 면적 비율**
- 3 : **찰스강의 경계에 위치한 경우는 1, 아니면 0**
- 4 : **일산화질소 농도**
- 5 : **주택당 방 수 (거실 외 subroom)**
- 6 : **1940년 이전에 건축된 주택의 비율**
- 7 : **직업센터의 거리**
- 8 : **방사형 고속도로까지의 거리**
- 9 : **재산세율**
- 10 : **학생/교사 비율**
- 11 : **인구 중 흑인 비율**
- 12 : **인구 중 하위 계층 비율**

```python
boston_X = boston_data[:,(5, 12)] # 주택당 방 수(5) & 인구 중 하위 계층 비율(12)
boston_X
boston_Y = boston_target
```

## 🐌 **3. Training & Test set 으로 나눠주기**

```python
from sklearn import model_selection

x_train, x_test, y_train, y_test = model_selection.train_test_split(boston_X, boston_Y, test_size=0.3, random_state=0)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

>>> (354, 2)
		(152, 2)
		(354,)
		(152,)
```

## 🐌 **4. 비어있는 모델 객체 만들기**

```python
model = linear_model.LogisticRegression() # 로지스틱회귀
```

## 🐌 **5. 모델 객체 학습시키기 (on training data)**

```python
model.fit(x_train, y_train)
```

## 🐌 **6. 학습이 끝난 모델 테스트하기 (on test data)**

```python
pred_test_ = model.predict(x_test)
pred_test_

array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,
       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
       1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
       0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,
       1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,
       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,
       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1])
```

```python
# 양성/음성 확률을 확인하려면?

# plot roc curve for test set
pred_test = model.predict_proba(x_test) # Predict 'probability'
pred_test

array([[2.71727138e-01, 7.28272862e-01],
       [5.14126251e-01, 4.85873749e-01],
       [5.72011266e-01, 4.27988734e-01],
       [9.94933971e-01, 5.06602928e-03],
       [7.70744299e-01, 2.29255701e-01],
       [5.07806637e-01, 4.92193363e-01],
       [7.68191608e-01, 2.31808392e-01],
       [5.56571586e-01, 4.43428414e-01],
       [9.37812032e-01, 6.21879678e-02],
       [5.76735788e-01, 4.23264212e-01],
       [9.93110093e-01, 6.88990666e-03],
       [9.88530491e-01, 1.14695093e-02],
       [9.66734148e-01, 3.32658524e-02],
       [9.99931971e-01, 6.80287144e-05],
       [7.73411904e-03, 9.92265881e-01],
       [5.49287966e-02, 9.45071203e-01],
       [6.30280147e-01, 3.69719853e-01],
       [3.26213034e-02, 9.67378697e-01],
       [1.13247007e-01, 8.86752993e-01],
       [5.34701631e-01, 4.65298369e-01],
       [2.41228758e-01, 7.58771242e-01],
       [8.12700869e-01, 1.87299131e-01],
       [8.86759215e-01, 1.13240785e-01],
       [2.29468197e-01, 7.70531803e-01],
       [7.15844986e-01, 2.84155014e-01],
       [9.85433064e-01, 1.45669362e-02],
       [8.93450425e-01, 1.06549575e-01],
       [9.61753263e-01, 3.82467368e-02],
       [1.49239635e-02, 9.85076036e-01],
       [9.66076142e-01, 3.39238577e-02],
       [9.74420812e-01, 2.55791880e-02],
       [9.13966573e-01, 8.60334271e-02],
       [5.75544988e-01, 4.24455012e-01],
       [8.00154741e-01, 1.99845259e-01],
       [4.09355893e-01, 5.90644107e-01],
       [9.33500378e-01, 6.64996221e-02],
       [9.99450691e-01, 5.49309494e-04],
       [5.39331552e-01, 4.60668448e-01],
       [9.64398012e-01, 3.56019882e-02],
       [9.97974838e-01, 2.02516166e-03],
       [4.39555425e-01, 5.60444575e-01],
       [9.20976636e-01, 7.90233640e-02],
       [4.87862293e-01, 5.12137707e-01],
       [9.89166226e-01, 1.08337737e-02],
       [3.39622078e-01, 6.60377922e-01],
       [4.18160120e-01, 5.81839880e-01],
       [9.59186762e-01, 4.08132381e-02],
       [9.23155145e-01, 7.68448546e-02],
       [9.99941302e-01, 5.86975360e-05],
       [3.66389636e-01, 6.33610364e-01],
       [9.18273583e-01, 8.17264175e-02],
       [9.82418719e-01, 1.75812813e-02],
       [7.68277772e-01, 2.31722228e-01],
       [9.21976763e-03, 9.90780232e-01],
       [9.78268469e-01, 2.17315308e-02],
       [9.52330598e-01, 4.76694016e-02],
       [8.61142582e-01, 1.38857418e-01],
       [8.47517134e-01, 1.52482866e-01],
       [6.90025318e-01, 3.09974682e-01],
       [9.72575668e-01, 2.74243322e-02],
       [4.23755620e-01, 5.76244380e-01],
       [8.18379345e-01, 1.81620655e-01],
       [6.11890463e-02, 9.38810954e-01],
       [6.17920841e-02, 9.38207916e-01],
       [9.42143532e-01, 5.78564681e-02],
       [8.39015452e-02, 9.16098455e-01],
       [9.52630943e-01, 4.73690565e-02],
       [9.88351919e-01, 1.16480806e-02],
       [9.74471838e-01, 2.55281623e-02],
       [7.25360906e-01, 2.74639094e-01],
       [7.32364267e-01, 2.67635733e-01],
       [4.33803079e-01, 5.66196921e-01],
       [1.13657837e-01, 8.86342163e-01],
       [6.19749132e-02, 9.38025087e-01],
       [2.18496568e-01, 7.81503432e-01],
       [9.99978866e-01, 2.11338164e-05],
       [1.10653228e-02, 9.88934677e-01],
       [6.31274937e-01, 3.68725063e-01],
       [3.86091547e-01, 6.13908453e-01],
       [9.18793780e-01, 8.12062197e-02],
       [2.18978410e-01, 7.81021590e-01],
       [8.70971996e-01, 1.29028004e-01],
       [8.58951047e-01, 1.41048953e-01],
       [1.17251361e-02, 9.88274864e-01],
       [8.73969119e-03, 9.91260309e-01],
       [1.88484605e-01, 8.11515395e-01],
       [7.04808041e-01, 2.95191959e-01],
       [9.54483076e-01, 4.55169244e-02],
       [2.38960385e-01, 7.61039615e-01],
       [9.83557833e-01, 1.64421667e-02],
       [9.31864641e-01, 6.81353594e-02],
       [9.95242281e-01, 4.75771865e-03],
       [2.57003724e-01, 7.42996276e-01],
       [1.08702780e-01, 8.91297220e-01],
       [3.94291078e-01, 6.05708922e-01],
       [5.05001316e-01, 4.94998684e-01],
       [9.99999322e-01, 6.78257496e-07],
       [1.54968361e-01, 8.45031639e-01],
       [9.64781841e-01, 3.52181591e-02],
       [8.89657715e-01, 1.10342285e-01],
       [4.55159528e-01, 5.44840472e-01],
       [7.14813079e-01, 2.85186921e-01],
       [3.38189397e-01, 6.61810603e-01],
       [6.18054035e-01, 3.81945965e-01],
       [5.21245611e-01, 4.78754389e-01],
       [8.16149714e-01, 1.83850286e-01],
       [9.99724943e-01, 2.75056881e-04],
       [9.64162388e-01, 3.58376118e-02],
       [4.93371890e-01, 5.06628110e-01],
       [3.52638932e-01, 6.47361068e-01],
       [5.72815671e-02, 9.42718433e-01],
       [9.99572037e-01, 4.27962534e-04],
       [9.84046418e-01, 1.59535815e-02],
       [9.46404747e-01, 5.35952528e-02],
       [9.99900466e-01, 9.95342786e-05],
       [9.14113492e-01, 8.58865076e-02],
       [9.99974854e-01, 2.51462387e-05],
       [6.34152244e-01, 3.65847756e-01],
       [9.99693557e-01, 3.06443474e-04],
       [1.13320929e-02, 9.88667907e-01],
       [7.33633543e-02, 9.26636646e-01],
       [9.99170311e-01, 8.29689161e-04],
       [9.71050561e-01, 2.89494389e-02],
       [8.43889315e-01, 1.56110685e-01],
       [6.24070775e-01, 3.75929225e-01],
       [9.77827418e-01, 2.21725818e-02],
       [7.33144904e-02, 9.26685510e-01],
       [9.88494358e-01, 1.15056416e-02],
       [5.60409345e-01, 4.39590655e-01],
       [7.27574818e-02, 9.27242518e-01],
       [9.50147850e-01, 4.98521498e-02],
       [9.98031324e-01, 1.96867584e-03],
       [9.79716873e-01, 2.02831268e-02],
       [9.22397735e-01, 7.76022650e-02],
       [9.92672947e-01, 7.32705321e-03],
       [4.55097311e-02, 9.54490269e-01],
       [6.81859400e-01, 3.18140600e-01],
       [9.99715718e-01, 2.84282026e-04],
       [1.50298199e-01, 8.49701801e-01],
       [9.99654459e-01, 3.45541085e-04],
       [9.96953381e-01, 3.04661883e-03],
       [8.87840078e-01, 1.12159922e-01],
       [6.57864490e-02, 9.34213551e-01],
       [1.15002142e-01, 8.84997858e-01],
       [2.39726039e-01, 7.60273961e-01],
       [9.37133513e-01, 6.28664865e-02],
       [4.44028114e-02, 9.55597189e-01],
       [6.21144479e-02, 9.37885552e-01],
       [9.93872229e-01, 6.12777127e-03],
       [9.99953182e-01, 4.68184743e-05],
       [7.69059703e-02, 9.23094030e-01],
       [1.21173269e-01, 8.78826731e-01]])
```

```python
from sklearn.metrics import accuracy_score # accuracy

# 모델 분류의 정확도
print('Accuracy: ', accuracy_score(model.predict(x_test), y_test))

>>> Accuracy:  0.8223684210526315
```

---

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ad8b0c0d-1158-4983-b851-c518406e8b7a/Untitled.png)

1. 암 환자 판별 문제

암환자가 맞는데 아니라고 판별 하는 False Negative(FN)가 가장 중요하므로 FN값을 줄이기 위해 노력

1. 스팸 판별 문제

스팸이 아닌데 스팸이라고 판별 하는 False positive (FP)는 중요한 문자가 왔을 때 스팸이라고 처리해서 못 읽게 할 수 있기 때문에 FP를 줄이도록 노력

## 🐌 Recall(재현율)

실제 정답 positive 중에서(실제 참인 것들 중) TruePositive의 확률(FN 중요: 확률을 높이기 위해)

→ TP/TP+FN

## 🐌 Precision(정밀도)

예측 positive중에서(참 이라고 예측 한 것 중) 진짜 TP의 확률(FP 중요)

→ TP/Tp+FP

## 🐌 F1-Score

: Recall(R)과 Precision(P)의 조화 평균

→ **2*R*P/R+P**

### 🐌 **F-Beta Score**

: F1-Score은 R과 P를 동등하게 두고 구하지만, F-Beta score은 R과 P중 어느 것에 더 비중을 둘 지 

  가중치를 적용할 수 있다.
