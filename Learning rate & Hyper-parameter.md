# ğŸŒ Gradient Descent Algorithm (ê²½ì‚¬ í•˜ê°•ë²•)

- Gradient: ëª¨ë“  ë³€ìˆ˜ì˜ í¸ë¯¸ë¶„ì„ ë²¡í„°ë¡œ ì •ë¦¬í•œ ê²ƒ(= í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°, ê²½ì‚¬)
- í¸ë¯¸ë¶„: ë³€ìˆ˜ê°€ 2ê°œ ì´ìƒì¸ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•  ë•Œ ë¯¸ë¶„ ëŒ€ìƒ ë³€ìˆ˜ ì™¸ì— ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë¥¼ ìƒìˆ˜ì²˜ëŸ¼ ê³ ì •ì‹œì¼œ ë¯¸ë¶„í•˜ëŠ” ê²ƒ
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ed8376dd-9fa7-4c49-bc9d-0a18298ab5cf/Untitled.png)
    

1) ë³€ìˆ˜(Î¸)ì˜ ì´ˆê¸°ê°’ì„ ì„¤ì •

2) í˜„ì¬ ë³€ìˆ˜ ê°’ì— ëŒ€ì‘ë˜ëŠ” Cost functionì˜ ê²½ì‚¬ë„ ê³„ì‚°(ë¯¸ë¶„)

3) ë³€ìˆ˜ë¥¼ ê²½ì‚¬ ë°©í–¥(ê¸°ìš¸ê¸°ì˜ ìŒì˜ ë°©í–¥ = Gradient ì˜ ìŒì˜ ë°©í–¥)ìœ¼ë¡œ ì›€ì§ì—¬ ë‹¤ìŒ ë³€ìˆ˜ ê°’ìœ¼ë¡œ ì„¤ì •

4) 1~3 ì„ ë°˜ë³µí•˜ë©° Cost functionì´ ìµœì†Œê°€ ë˜;ë„ë¡ í•˜ëŠ” ë³€ìˆ˜ ê°’ìœ¼ë¡œ ê·¼ì ‘í•´ ë‚˜ê°„ë‹¤.

(=ì „ì²´ Cost ê°’ì´ ë³€í•˜ì§€ ì•Šê±°ë‚˜ ë§¤ìš° ëŠë¦¬ê²Œ ë³€í•  ë•Œ ê¹Œì§€ ì ‘ê·¼)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/348df093-b351-44c8-bc58-52d5e13b0e5d/Untitled.png)

**í•™ìŠµë¥  í…ŒìŠ¤íŠ¸**

      â‡’ https://developers.google.com/machine-learning/crash-course/fitter/graph?hl=ko

**Hyper-parameter** : ì´ˆë§¤ê°œë³€ìˆ˜ ( ì‚¬ëŒì´ ì„¤ì •í•˜ëŠ” ëª¨ë“  ê²ƒ)

**AutoML**

Auto F.E

Auto M.S

Auto HPO (hyper-parameter tuning / model tuning)
